{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models_lib.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hossein20s/Average_Blood_Pressure_estimation/blob/master/models_lib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpVnnY7_N6b8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Version = '3.5'\n",
        "print('models.lib.ipynb Version: ', Version)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o081Ube71gi",
        "colab_type": "text"
      },
      "source": [
        "# Important\n",
        "\n",
        "Remember to change all keras. to tensorflow.keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIGHaGDdRUyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime as dt\n",
        "\n",
        "class Timer():\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tself.start_dt = None\n",
        "\n",
        "\tdef start(self):\n",
        "\t\tself.start_dt = dt.datetime.now()\n",
        "\n",
        "\tdef stop(self):\n",
        "\t\tend_dt = dt.datetime.now()\n",
        "\t\tprint('Time taken: %s' % (end_dt - self.start_dt))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3mUuq_CNG5A",
        "colab_type": "code",
        "outputId": "3dee7d42-6183-4fc2-bcc5-a79ca69a0c02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import RepeatVector\n",
        "\n",
        "# define the encoder-decoder model\n",
        "def baseline_model(n_timesteps_in, n_features):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(LSTM(150, input_shape=(n_timesteps_in, n_features)))\n",
        "\tmodel.add(RepeatVector(n_timesteps_in))\n",
        "\tmodel.add(LSTM(150, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(n_features, activation='softmax')))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\treturn model\n",
        "\n",
        "# define the encoder-decoder with attention model\n",
        "def attention_model(n_timesteps_in, n_features):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(LSTM(150, input_shape=(n_timesteps_in, n_features), return_sequences=True))\n",
        "\tmodel.add(AttentionDecoder(150, n_features))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\treturn model"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bxRxL00hs6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train and evaluate a model, return accuracy\n",
        "def train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features):\n",
        "\t# train LSTM\n",
        "\tfor epoch in range(5000):\n",
        "\t\t# generate new random sequence\n",
        "\t\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\t\t# fit model for one epoch on this sequence\n",
        "\t\tmodel.fit(X, y, epochs=1, verbose=0)\n",
        "\t# evaluate LSTM\n",
        "\ttotal, correct = 100, 0\n",
        "\tfor _ in range(total):\n",
        "\t\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\t\tyhat = model.predict(X, verbose=0)\n",
        "\t\tif array_equal(one_hot_decode(y[0]), one_hot_decode(yhat[0])):\n",
        "\t\t\tcorrect += 1\n",
        "\treturn float(correct)/float(total)*100.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STQrave6ayYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from numpy import newaxis\n",
        "from keras.layers import Dense, Activation, Dropout, LSTM\n",
        "from keras import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import RepeatVector\n",
        "\n",
        "class TheModel():\n",
        "  \"\"\"A class for an building and inferencing an lstm model\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.model = Sequential()\n",
        "\n",
        "  def load_model(self, filepath):\n",
        "    print('[Model] Loading model from file %s' % filepath)\n",
        "    self.model = load_model(filepath)\n",
        "\n",
        "  def build_model(self, configs):\n",
        "    timer = Timer()\n",
        "    timer.start()\n",
        "    timer.start()\n",
        "    \n",
        "    has_input_shape = False\n",
        "\n",
        "    for layer in configs['model']['layers']:\n",
        "      neurons = layer['neurons'] if 'neurons' in layer else None\n",
        "      dropout_rate = layer['rate'] if 'rate' in layer else None\n",
        "      activation = layer['activation'] if 'activation' in layer else None\n",
        "      return_seq = layer['return_seq'] if 'return_seq' in layer else None\n",
        "      input_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None\n",
        "      input_dim = layer['input_dim'] if 'input_dim' in layer else None\n",
        "      if 'input_shape' in layer:\n",
        "        input_shape = layer['input_shape']\n",
        "        has_input_shape = True  \n",
        "\n",
        "      if layer['type'] == 'dense':\n",
        "        if(input_shape is not None):\n",
        "          self.model.add(Dense(neurons, activation=activation, input_shape=input_shape))\n",
        "        else:\n",
        "          self.model.add(Dense(neurons, activation=activation))\n",
        "      if layer['type'] == 'lstm':\n",
        "        input_shape=(input_timesteps, input_dim)\n",
        "        has_input_shape = True\n",
        "        self.model.add(LSTM(neurons, input_shape=input_shape, return_sequences=return_seq))\n",
        "      if layer['type'] == 'dropout':\n",
        "        self.model.add(Dropout(dropout_rate))\n",
        "      if layer['type'] == 'repeat_vector':\n",
        "        self.model.add(RepeatVector(input_timesteps))\n",
        "      if layer['type'] == 'time_distributed_dense':\n",
        "        self.model.add(TimeDistributed(Dense(neurons, activation=activation)))\n",
        "      if layer['type'] == 'attention_decoder':\n",
        "        self.model.add(AttentionDecoder(neurons, input_dim))\n",
        "      if layer['type'] == 'attention':\n",
        "        self.model.add(Attention())\n",
        "\n",
        "    self.model.compile(loss=configs['model']['loss'], \n",
        "                       optimizer=configs['model']['optimizer'],\n",
        "                      metrics=configs['model']['metrics'])\n",
        "    print('[Model] Model Compiled')\n",
        "    print('input_shape: ',input_shape)\n",
        "    if has_input_shape:\n",
        "      self.model.summary()\n",
        "    timer.stop()\n",
        "\n",
        "  def train(self, x, y, epochs, batch_size, save_dir):\n",
        "    timer = Timer()\n",
        "    timer.start()\n",
        "    print('[Model] Training Started')\n",
        "    print('[Model] %s epochs, %s batch size' % (epochs, batch_size))\n",
        "\n",
        "    save_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n",
        "\n",
        "    callbacks = [\n",
        "        PrintDot()\n",
        "    #\t\t\tEarlyStopping(monitor='val_loss', patience=2),\n",
        "    #\t\t\tModelCheckpoint(filepath=save_fname, monitor='val_loss', save_best_only=True)\n",
        "    ]\n",
        "    history = self.model.fit(\n",
        "      x,\n",
        "      y,\n",
        "      epochs=epochs,\n",
        "      batch_size=batch_size,\n",
        "      validation_split = 0.2, \n",
        "      verbose=0,\n",
        "      callbacks=callbacks\n",
        "    )\n",
        "    self.model.save(save_fname)\n",
        "\n",
        "    print('[Model] Training Completed. Model saved as %s' % save_fname)\n",
        "    timer.stop()\n",
        "    return history\n",
        "\n",
        "  def train_generator(self, data_gen, epochs, batch_size, steps_per_epoch, save_dir):\n",
        "    timer = Timer()\n",
        "    timer.start()\n",
        "    print('[Model] Training Started')\n",
        "    print('[Model] %s epochs, %s batch size, %s batches per epoch' % (epochs, batch_size, steps_per_epoch))\n",
        "\n",
        "    save_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n",
        "    callbacks = [\n",
        "      ModelCheckpoint(filepath=save_fname, monitor='loss', save_best_only=True)\n",
        "    ]\n",
        "    self.model.fit_generator(\n",
        "      data_gen,\n",
        "      steps_per_epoch=steps_per_epoch,\n",
        "      epochs=epochs,\n",
        "      callbacks=callbacks,\n",
        "      workers=1\n",
        "    )\n",
        "\n",
        "    print('[Model] Training Completed. Model saved as %s' % save_fname)\n",
        "    timer.stop()\n",
        "\n",
        "  def predict_point_by_point(self, data):\n",
        "    #Predict each timestep given the last sequence of true data, in effect only predicting 1 step ahead each time\n",
        "    print('[Model] Predicting Point-by-Point...')\n",
        "    predicted = self.model.predict(data)\n",
        "    predicted = np.reshape(predicted, (predicted.size,))\n",
        "    return predicted\n",
        "\n",
        "  def predict_sequences_multiple(self, data, window_size, prediction_len):\n",
        "    #Predict sequence of 50 steps before shifting prediction run forward by 50 steps\n",
        "    print('[Model] Predicting Sequences Multiple...')\n",
        "    prediction_seqs = []\n",
        "    for i in range(int(len(data)/prediction_len)):\n",
        "      curr_frame = data[i*prediction_len]\n",
        "      predicted = []\n",
        "      for j in range(prediction_len):\n",
        "        predicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n",
        "        curr_frame = curr_frame[1:]\n",
        "        curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
        "      prediction_seqs.append(predicted)\n",
        "    return prediction_seqs\n",
        "\n",
        "  def predict_sequence_full(self, data, window_size):\n",
        "    #Shift the window by 1 new prediction each time, re-run predictions on new window\n",
        "    print('[Model] Predicting Sequences Full...')\n",
        "    curr_frame = data[0]\n",
        "    predicted = []\n",
        "    for i in range(len(data)):\n",
        "      predicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n",
        "      curr_frame = curr_frame[1:]\n",
        "      curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
        "    return predicted\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRPd4bsBJFD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras import backend as K, initializers, regularizers, constraints\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "\n",
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        # todo: check that this is correct\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True,\n",
        "                 return_attention=False,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "\n",
        "\n",
        "        Note: The layer has been tested with Keras 1.x\n",
        "\n",
        "        Example:\n",
        "        \n",
        "            # 1\n",
        "            model.add(LSTM(64, return_sequences=True))\n",
        "            model.add(Attention())\n",
        "            # next add a Dense layer (for classification/regression) or whatever...\n",
        "\n",
        "            # 2 - Get the attention scores\n",
        "            hidden = LSTM(64, return_sequences=True)(words)\n",
        "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
        "\n",
        "        \"\"\"\n",
        "        self.supports_masking = True\n",
        "        self.return_attention = return_attention\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        eij = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        weighted_input = x * K.expand_dims(a)\n",
        "\n",
        "        result = K.sum(weighted_input, axis=1)\n",
        "\n",
        "        if self.return_attention:\n",
        "            return [result, a]\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.return_attention:\n",
        "            return [(input_shape[0], input_shape[-1]),\n",
        "                    (input_shape[0], input_shape[1])]\n",
        "        else:\n",
        "            return input_shape[0], input_shape[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rVaMx5IOnrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display training progress by printing a single dot for each completed epoch\n",
        "class PrintDot(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    if epoch % 100 == 0: print('')\n",
        "    print('.', end='')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V02JQsQsWl2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "from keras import regularizers, constraints, initializers, activations\n",
        "from keras.layers.recurrent import Recurrent\n",
        "from keras.engine import InputSpec\n",
        "\n",
        "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
        "\n",
        "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
        "                            input_dim=None, output_dim=None,\n",
        "                            timesteps=None, training=None):\n",
        "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
        "    # Arguments\n",
        "        x: input tensor.\n",
        "        w: weight matrix.\n",
        "        b: optional bias vector.\n",
        "        dropout: wether to apply dropout (same dropout mask\n",
        "            for every temporal slice of the input).\n",
        "        input_dim: integer; optional dimensionality of the input.\n",
        "        output_dim: integer; optional dimensionality of the output.\n",
        "        timesteps: integer; optional number of timesteps.\n",
        "        training: training phase tensor or boolean.\n",
        "    # Returns\n",
        "        Output tensor.\n",
        "    \"\"\"\n",
        "    if not input_dim:\n",
        "        input_dim = K.shape(x)[2]\n",
        "    if not timesteps:\n",
        "        timesteps = K.shape(x)[1]\n",
        "    if not output_dim:\n",
        "        output_dim = K.shape(w)[1]\n",
        "\n",
        "    if dropout is not None and 0. < dropout < 1.:\n",
        "        # apply the same dropout pattern at every timestep\n",
        "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
        "        dropout_matrix = K.dropout(ones, dropout)\n",
        "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
        "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
        "\n",
        "    # collapse time dimension and batch dimension together\n",
        "    x = K.reshape(x, (-1, input_dim))\n",
        "    x = K.dot(x, w)\n",
        "    if b is not None:\n",
        "        x = K.bias_add(x, b)\n",
        "    # reshape to 3D tensor\n",
        "    if K.backend() == 'tensorflow':\n",
        "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
        "        x.set_shape([None, None, output_dim])\n",
        "    else:\n",
        "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
        "    return x\n",
        "\n",
        "class AttentionDecoder(Layer):\n",
        "\n",
        "    def __init__(self, units, output_dim,\n",
        "                 activation='tanh',\n",
        "                 return_probabilities=False,\n",
        "                 name='AttentionDecoder',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
        "        encoder and outputs the decoded states \n",
        "        :param units: dimension of the hidden state and the attention matrices\n",
        "        :param output_dim: the number of labels in the output space\n",
        "        references:\n",
        "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \n",
        "            \"Neural machine translation by jointly learning to align and translate.\" \n",
        "            arXiv preprint arXiv:1409.0473 (2014).\n",
        "        \"\"\"\n",
        "        self.units = units\n",
        "        self.output_dim = output_dim\n",
        "        self.return_probabilities = return_probabilities\n",
        "        self.activation = activations.get(activation)\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        super(AttentionDecoder, self).__init__(**kwargs)\n",
        "        self.name = name\n",
        "        self.return_sequences = True  # must return sequences\n",
        "\n",
        "\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        super(MyLayer, self).__init__(**kwargs) \n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
        "          for model details that correspond to the matrices here.\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
        "\n",
        "        if self.stateful:\n",
        "            super(AttentionDecoder, self).reset_states()\n",
        "\n",
        "        self.states = [None, None]  # y, s\n",
        "\n",
        "        \"\"\"\n",
        "            Matrices for creating the context vector\n",
        "        \"\"\"\n",
        "\n",
        "        self.V_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='V_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='W_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='U_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.b_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='b_a',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the r (reset) gate\n",
        "        \"\"\"\n",
        "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_r = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_r',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "\n",
        "        \"\"\"\n",
        "            Matrices for the z (update) gate\n",
        "        \"\"\"\n",
        "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_z = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_z',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the proposal\n",
        "        \"\"\"\n",
        "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_p = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_p',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for making the final prediction vector\n",
        "        \"\"\"\n",
        "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
        "                                   name='C_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
        "                                   name='U_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
        "                                   name='W_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
        "                                   name='b_o',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "\n",
        "        # For creating the initial state:\n",
        "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='W_s',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "\n",
        "        self.input_spec = [\n",
        "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, x):\n",
        "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
        "        self.x_seq = x\n",
        "\n",
        "        # apply the a dense layer over the time dimension of the sequence\n",
        "        # do it here because it doesn't depend on any previous steps\n",
        "        # thefore we can save computation time:\n",
        "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
        "                                             input_dim=self.input_dim,\n",
        "                                             timesteps=self.timesteps,\n",
        "                                             output_dim=self.units)\n",
        "\n",
        "        return super(AttentionDecoder, self).call(x)\n",
        "\n",
        "    def get_initial_state(self, inputs):\n",
        "        print('inputs shape:', inputs.get_shape())\n",
        "\n",
        "        # apply the matrix on the first time step to get the initial s0.\n",
        "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
        "\n",
        "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
        "        # output_dim)\n",
        "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
        "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
        "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
        "        y0 = K.tile(y0, [1, self.output_dim])\n",
        "\n",
        "        return [y0, s0]\n",
        "\n",
        "    def step(self, x, states):\n",
        "\n",
        "        ytm, stm = states\n",
        "\n",
        "        # repeat the hidden state to the length of the sequence\n",
        "        _stm = K.repeat(stm, self.timesteps)\n",
        "\n",
        "        # now multiplty the weight matrix with the repeated hidden state\n",
        "        _Wxstm = K.dot(_stm, self.W_a)\n",
        "\n",
        "        # calculate the attention probabilities\n",
        "        # this relates how much other timesteps contributed to this one.\n",
        "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
        "                   K.expand_dims(self.V_a))\n",
        "        at = K.exp(et)\n",
        "        at_sum = K.sum(at, axis=1)\n",
        "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
        "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
        "\n",
        "        # calculate the context vector\n",
        "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
        "        # ~~~> calculate new hidden state\n",
        "        # first calculate the \"r\" gate:\n",
        "\n",
        "        rt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_r)\n",
        "            + K.dot(stm, self.U_r)\n",
        "            + K.dot(context, self.C_r)\n",
        "            + self.b_r)\n",
        "\n",
        "        # now calculate the \"z\" gate\n",
        "        zt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_z)\n",
        "            + K.dot(stm, self.U_z)\n",
        "            + K.dot(context, self.C_z)\n",
        "            + self.b_z)\n",
        "\n",
        "        # calculate the proposal hidden state:\n",
        "        s_tp = activations.tanh(\n",
        "            K.dot(ytm, self.W_p)\n",
        "            + K.dot((rt * stm), self.U_p)\n",
        "            + K.dot(context, self.C_p)\n",
        "            + self.b_p)\n",
        "\n",
        "        # new hidden state:\n",
        "        st = (1-zt)*stm + zt * s_tp\n",
        "\n",
        "        yt = activations.softmax(\n",
        "            K.dot(ytm, self.W_o)\n",
        "            + K.dot(stm, self.U_o)\n",
        "            + K.dot(context, self.C_o)\n",
        "            + self.b_o)\n",
        "\n",
        "        if self.return_probabilities:\n",
        "            return at, [yt, st]\n",
        "        else:\n",
        "            return yt, [yt, st]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "            For Keras internal compatability checking\n",
        "        \"\"\"\n",
        "        if self.return_probabilities:\n",
        "            return (None, self.timesteps, self.timesteps)\n",
        "        else:\n",
        "            return (None, self.timesteps, self.output_dim)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "            For rebuilding models on load time.\n",
        "        \"\"\"\n",
        "        config = {\n",
        "            'output_dim': self.output_dim,\n",
        "            'units': self.units,\n",
        "            'return_probabilities': self.return_probabilities\n",
        "        }\n",
        "        base_config = super(AttentionDecoder, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4twSlb-4tyP",
        "colab_type": "text"
      },
      "source": [
        "GOt this error with AttentionDecoder\n",
        "\n",
        "TypeError: The added layer must be an instance of class Layer. Found: <__main__.AttentionDecoder object at 0x7ff4db534860>\n",
        "\n",
        "I tried to fix it following https://keras.io/layers/writing-your-own-keras-layers/ but it didnt help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LMFPujgZ5Zyv",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "from keras import regularizers, constraints, initializers, activations\n",
        "from keras.layers.recurrent import Recurrent\n",
        "from keras.engine import InputSpec\n",
        "\n",
        "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
        "\n",
        "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
        "                            input_dim=None, output_dim=None,\n",
        "                            timesteps=None, training=None):\n",
        "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
        "    # Arguments\n",
        "        x: input tensor.\n",
        "        w: weight matrix.\n",
        "        b: optional bias vector.\n",
        "        dropout: wether to apply dropout (same dropout mask\n",
        "            for every temporal slice of the input).\n",
        "        input_dim: integer; optional dimensionality of the input.\n",
        "        output_dim: integer; optional dimensionality of the output.\n",
        "        timesteps: integer; optional number of timesteps.\n",
        "        training: training phase tensor or boolean.\n",
        "    # Returns\n",
        "        Output tensor.\n",
        "    \"\"\"\n",
        "    if not input_dim:\n",
        "        input_dim = K.shape(x)[2]\n",
        "    if not timesteps:\n",
        "        timesteps = K.shape(x)[1]\n",
        "    if not output_dim:\n",
        "        output_dim = K.shape(w)[1]\n",
        "\n",
        "    if dropout is not None and 0. < dropout < 1.:\n",
        "        # apply the same dropout pattern at every timestep\n",
        "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
        "        dropout_matrix = K.dropout(ones, dropout)\n",
        "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
        "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
        "\n",
        "    # collapse time dimension and batch dimension together\n",
        "    x = K.reshape(x, (-1, input_dim))\n",
        "    x = K.dot(x, w)\n",
        "    if b is not None:\n",
        "        x = K.bias_add(x, b)\n",
        "    # reshape to 3D tensor\n",
        "    if K.backend() == 'tensorflow':\n",
        "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
        "        x.set_shape([None, None, output_dim])\n",
        "    else:\n",
        "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
        "    return x\n",
        "\n",
        "class AttentionDecoder(Layer):\n",
        "\n",
        "    def __init__1(self, units, output_dim,\n",
        "                 activation='tanh',\n",
        "                 return_probabilities=False,\n",
        "                 name='AttentionDecoder',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
        "        encoder and outputs the decoded states \n",
        "        :param units: dimension of the hidden state and the attention matrices\n",
        "        :param output_dim: the number of labels in the output space\n",
        "        references:\n",
        "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \n",
        "            \"Neural machine translation by jointly learning to align and translate.\" \n",
        "            arXiv preprint arXiv:1409.0473 (2014).\n",
        "        \"\"\"\n",
        "        self.units = units\n",
        "        self.output_dim = output_dim\n",
        "        self.return_probabilities = return_probabilities\n",
        "        self.activation = activations.get(activation)\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        super(AttentionDecoder, self).__init__(**kwargs)\n",
        "        self.name = name\n",
        "        self.return_sequences = True  # must return sequences\n",
        "\n",
        "\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        super(AttentionDecoder, self).__init__(**kwargs) \n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        self.kernel = self.add_weight(name='kernel', \n",
        "                                      shape=(input_shape[1], self.output_dim),\n",
        "                                      initializer='uniform',\n",
        "                                      trainable=True)\n",
        "        super(AttentionDecoder, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "        \n",
        "    def build1(self, input_shape):\n",
        "        \"\"\"\n",
        "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
        "          for model details that correspond to the matrices here.\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
        "\n",
        "        if self.stateful:\n",
        "            super(AttentionDecoder, self).reset_states()\n",
        "\n",
        "        self.states = [None, None]  # y, s\n",
        "\n",
        "        \"\"\"\n",
        "            Matrices for creating the context vector\n",
        "        \"\"\"\n",
        "\n",
        "        self.V_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='V_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='W_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='U_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.b_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='b_a',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the r (reset) gate\n",
        "        \"\"\"\n",
        "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_r = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_r',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "\n",
        "        \"\"\"\n",
        "            Matrices for the z (update) gate\n",
        "        \"\"\"\n",
        "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_z = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_z',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the proposal\n",
        "        \"\"\"\n",
        "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_p = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_p',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for making the final prediction vector\n",
        "        \"\"\"\n",
        "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
        "                                   name='C_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
        "                                   name='U_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
        "                                   name='W_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
        "                                   name='b_o',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "\n",
        "        # For creating the initial state:\n",
        "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='W_s',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "\n",
        "        self.input_spec = [\n",
        "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, x):\n",
        "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
        "        self.x_seq = x\n",
        "\n",
        "        # apply the a dense layer over the time dimension of the sequence\n",
        "        # do it here because it doesn't depend on any previous steps\n",
        "        # thefore we can save computation time:\n",
        "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
        "                                             input_dim=self.input_dim,\n",
        "                                             timesteps=self.timesteps,\n",
        "                                             output_dim=self.units)\n",
        "\n",
        "        return super(AttentionDecoder, self).call(x)\n",
        "\n",
        "    def get_initial_state(self, inputs):\n",
        "        print('inputs shape:', inputs.get_shape())\n",
        "\n",
        "        # apply the matrix on the first time step to get the initial s0.\n",
        "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
        "\n",
        "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
        "        # output_dim)\n",
        "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
        "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
        "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
        "        y0 = K.tile(y0, [1, self.output_dim])\n",
        "\n",
        "        return [y0, s0]\n",
        "\n",
        "    def step(self, x, states):\n",
        "\n",
        "        ytm, stm = states\n",
        "\n",
        "        # repeat the hidden state to the length of the sequence\n",
        "        _stm = K.repeat(stm, self.timesteps)\n",
        "\n",
        "        # now multiplty the weight matrix with the repeated hidden state\n",
        "        _Wxstm = K.dot(_stm, self.W_a)\n",
        "\n",
        "        # calculate the attention probabilities\n",
        "        # this relates how much other timesteps contributed to this one.\n",
        "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
        "                   K.expand_dims(self.V_a))\n",
        "        at = K.exp(et)\n",
        "        at_sum = K.sum(at, axis=1)\n",
        "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
        "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
        "\n",
        "        # calculate the context vector\n",
        "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
        "        # ~~~> calculate new hidden state\n",
        "        # first calculate the \"r\" gate:\n",
        "\n",
        "        rt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_r)\n",
        "            + K.dot(stm, self.U_r)\n",
        "            + K.dot(context, self.C_r)\n",
        "            + self.b_r)\n",
        "\n",
        "        # now calculate the \"z\" gate\n",
        "        zt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_z)\n",
        "            + K.dot(stm, self.U_z)\n",
        "            + K.dot(context, self.C_z)\n",
        "            + self.b_z)\n",
        "\n",
        "        # calculate the proposal hidden state:\n",
        "        s_tp = activations.tanh(\n",
        "            K.dot(ytm, self.W_p)\n",
        "            + K.dot((rt * stm), self.U_p)\n",
        "            + K.dot(context, self.C_p)\n",
        "            + self.b_p)\n",
        "\n",
        "        # new hidden state:\n",
        "        st = (1-zt)*stm + zt * s_tp\n",
        "\n",
        "        yt = activations.softmax(\n",
        "            K.dot(ytm, self.W_o)\n",
        "            + K.dot(stm, self.U_o)\n",
        "            + K.dot(context, self.C_o)\n",
        "            + self.b_o)\n",
        "\n",
        "        if self.return_probabilities:\n",
        "            return at, [yt, st]\n",
        "        else:\n",
        "            return yt, [yt, st]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "            For Keras internal compatability checking\n",
        "        \"\"\"\n",
        "        if self.return_probabilities:\n",
        "            return (None, self.timesteps, self.timesteps)\n",
        "        else:\n",
        "            return (None, self.timesteps, self.output_dim)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "            For rebuilding models on load time.\n",
        "        \"\"\"\n",
        "        config = {\n",
        "            'output_dim': self.output_dim,\n",
        "            'units': self.units,\n",
        "            'return_probabilities': self.return_probabilities\n",
        "        }\n",
        "        base_config = super(AttentionDecoder, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-7qDYFSYDfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras import backend as K, initializers, regularizers, constraints\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "\n",
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        # todo: check that this is correct\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True,\n",
        "                 return_attention=False,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "\n",
        "\n",
        "        Note: The layer has been tested with Keras 1.x\n",
        "\n",
        "        Example:\n",
        "        \n",
        "            # 1\n",
        "            model.add(LSTM(64, return_sequences=True))\n",
        "            model.add(Attention())\n",
        "            # next add a Dense layer (for classification/regression) or whatever...\n",
        "\n",
        "            # 2 - Get the attention scores\n",
        "            hidden = LSTM(64, return_sequences=True)(words)\n",
        "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
        "\n",
        "        \"\"\"\n",
        "        self.supports_masking = True\n",
        "        self.return_attention = return_attention\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        eij = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        weighted_input = x * K.expand_dims(a)\n",
        "\n",
        "        result = K.sum(weighted_input, axis=1)\n",
        "\n",
        "        if self.return_attention:\n",
        "            return [result, a]\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.return_attention:\n",
        "            return [(input_shape[0], input_shape[-1]),\n",
        "                    (input_shape[0], input_shape[1])]\n",
        "        else:\n",
        "            return input_shape[0], input_shape[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}