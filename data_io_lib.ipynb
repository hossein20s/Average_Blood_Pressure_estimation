{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_io_lib.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hossein20s/Average_Blood_Pressure_estimation/blob/master/data_io_lib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyMsKDUYsVX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Version = '3.4'\n",
        "print('data_io_lib Version: ', Version)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St2FlkauftTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class data_reader():\n",
        "    def __init__(self, filename, l=10, batchsize=32, random=True):\n",
        "        df = None\n",
        "        if(filename.endswith('.csv')):\n",
        "          df = pd.read_csv(filename)\n",
        "        elif(filename.endswith('.mat')):\n",
        "          with h5py.File(filename, 'r') as f:\n",
        "            print(list(f.keys()))\n",
        "            tmp = f['Part_1']\n",
        "            objectRef = tmp[0][0]\n",
        "            ds = f[objectRef]\n",
        "            dataframe = pd.DataFrame(index=index, columns=cols)\n",
        "            print(len(cols))\n",
        "            for i in range(len(cols)):\n",
        "              print(cols[i])\n",
        "              dataframe[cols[i]] = ds[:,i]\n",
        "            df = dataframe\n",
        "        # process the data into a matrix, and return the lenght\n",
        "        print(\"Warning: Data passed should be normalized!\")\n",
        "        self.frac = 0.65\n",
        "        self.random = random\n",
        "        self.process(df, l)\n",
        "        self.batchsize = batchsize\n",
        "\n",
        "        self.pointer = 0\n",
        "        self.epoch = 0\n",
        "    def process(self, df, l):\n",
        "        # Generate the data matrix\n",
        "        df = df[[\"NBP (Mean)\", \"Minute Volume\"]].dropna().as_matrix()\n",
        "        length = df.shape[0]\n",
        "        data = np.zeros((length-l, l))\n",
        "        label = np.zeros((length-l, 1))\n",
        "        for counter in range(length-l):\n",
        "            data[counter, :] = df[counter: counter+l, 1]\n",
        "            label[counter, :] = df[counter+l, 0]\n",
        "        # Random shuffle\n",
        "        length = data.shape[0]\n",
        "        idx = np.random.choice(length, length, replace=False)\n",
        "        if not self.random:\n",
        "            idx = np.arange(length)\n",
        "        self.val_idx = idx[int(self.frac*length):]\n",
        "\n",
        "        shuf_data = data[idx, :]\n",
        "        shuf_label = label[idx, :]\n",
        "        self.data =data\n",
        "        self.label = label\n",
        "\n",
        "        self.train_data = shuf_data[:int(self.frac*length), :]\n",
        "        self.train_label = shuf_label[:int(self.frac*length), :]\n",
        "        self.train_size = int(self.frac*length)\n",
        "\n",
        "        self.val_data = shuf_data[int(self.frac*length):, :]\n",
        "        self.val_label = shuf_label[int(self.frac*length):, :]\n",
        "        self.val_size = int((1-self.frac)*length)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_next_train_batch(self):\n",
        "        # getting the next train batch\n",
        "        if self.pointer + self.batchsize >= self.train_size:\n",
        "            end = self.train_size\n",
        "            start = self.pointer\n",
        "            self.pointer = 0\n",
        "            self.epoch += 1\n",
        "        else:\n",
        "            end = self.pointer + self.batchsize\n",
        "            start = self.pointer\n",
        "            self.pointer += self.batchsize\n",
        "        X = np.expand_dims(self.train_data[start:end, :], axis=-1)\n",
        "        Y = self.train_label[start:end, :]\n",
        "        return X, Y\n",
        "\n",
        "    def get_val(self):\n",
        "        X = np.expand_dims(self.val_data, axis=-1)\n",
        "\n",
        "        return X, self.val_label[:]\n",
        "\n",
        "    def get_whole(self):\n",
        "        # get whole, for validation set\n",
        "        X = np.expand_dims(self.data[:, :], axis=-1)\n",
        "        Y = self.label[:, :]\n",
        "        return X, Y\n",
        "\n",
        "    def reset(self):\n",
        "        self.pointer = 0\n",
        "        self.epoch = 0\n",
        "\n",
        "    def get_epoch(self):\n",
        "        # return the current epoch\n",
        "        return self.epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghAaEfogGnaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def normalize(df):\n",
        "  #stats = dataframe.describe()\n",
        "  return (df - df.mean()) / df.std()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txfaUXEuJd2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import sys\n",
        "\n",
        "class DataLoader():\n",
        "    \"\"\"A class for loading and transforming data for the lstm model\"\"\"\n",
        "\n",
        "    def __init__(self, filename, split, cols, index=None):\n",
        "      if(filename.endswith('.csv')):\n",
        "        dataframe = pd.read_csv(filename)\n",
        "      elif(filename.endswith('.mat')):\n",
        "        with h5py.File(filename, 'r') as f:\n",
        "          print(list(f.keys()))\n",
        "          tmp = f['Part_1']\n",
        "          objectRef = tmp[0][0]\n",
        "          ds = f[objectRef]\n",
        "          dataframe = pd.DataFrame(index=index, columns=cols)\n",
        "          print(len(cols))\n",
        "          for i in range(len(cols)):\n",
        "            print(cols[i])\n",
        "            dataframe[cols[i]] = ds[:,i]\n",
        "            \n",
        "      print(dataframe.shape)\n",
        "      dataframe = dataframe[:3000]\n",
        "      print(dataframe.shape)\n",
        "      print(dataframe.head(3))\n",
        "\n",
        "      i_split = int(len(dataframe) * split)\n",
        "      self.columns = cols\n",
        "      self.data_train = dataframe.get(cols).values[:i_split]\n",
        "      self.data_test  = dataframe.get(cols).values[i_split:]\n",
        "      self.len_train  = len(self.data_train)\n",
        "      self.len_test   = len(self.data_test)\n",
        "      self.len_train_windows = None\n",
        "\n",
        "    def get_train_data(self, seq_len, normalise, n_features):\n",
        "      return self._get_train_test_data(seq_len, normalise, self.data_train, n_features)\n",
        "    \n",
        "    def get_test_data(self, seq_len, normalise, n_features):\n",
        "      return self._get_train_test_data(seq_len, normalise, self.data_test, n_features)\n",
        "    \n",
        "    def _get_train_test_data(self, seq_len, normalise, data, n_features):\n",
        "        '''\n",
        "        Create x, y train data windows\n",
        "        Warning: batch method, not generative, make sure you have enough memory to\n",
        "        load data, otherwise use generate_training_window() method.\n",
        "        '''\n",
        "        data_x = []\n",
        "        data_y = []\n",
        "        for i in range(len(data) - 2*seq_len):\n",
        "            x, y = self._next_2window(i, seq_len, normalise, data, n_features)\n",
        "            data_x.append(x)\n",
        "            data_y.append(y)\n",
        "        return np.array(data_x), np.array(data_y)\n",
        "\n",
        "\n",
        "    def _next_window(self, i, seq_len, normalise, data, n_features):\n",
        "        '''Generates the next data window from the given index location i'''\n",
        "        window = data[i:i+seq_len]\n",
        "        normalized_window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
        "        x = normalized_window[:-1,:n_features]\n",
        "        y = window[-1:, [1]]\n",
        "        return x, y\n",
        "      \n",
        "    def _next_2window(self, i, seq_len, normalise, data, n_features):\n",
        "        '''Generates the next data window from the given index location i'''\n",
        "        window = data[i:i+2*seq_len]\n",
        "        normalized_window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
        "        x = normalized_window[:seq_len,:n_features]\n",
        "        y = window[seq_len:, [1]]\n",
        "        return x, y\n",
        "      \n",
        "\n",
        "    def xxget_test_data(self, seq_len, normalise):\n",
        "        '''\n",
        "        Create x, y test data windows\n",
        "        Warning: batch method, not generative, make sure you have enough memory to\n",
        "        load data, otherwise reduce size of the training split.\n",
        "        '''\n",
        "        data_windows = []\n",
        "        for i in range(self.len_test - seq_len):\n",
        "            data_windows.append(self.data_test[i:i+seq_len])\n",
        "\n",
        "        data_windows = np.array(data_windows).astype(float)\n",
        "        data_windows = self.normalise_windows(data_windows, single_window=False) if normalise else data_windows\n",
        "\n",
        "        x = data_windows[:, :-1]\n",
        "        y = data_windows[:, -1, [0]]\n",
        "        return x,y\n",
        "\n",
        "    def get_base_train_data(self, seq_len, normalise):\n",
        "        raw = pd.DataFrame( {  data.columns[0]: data.data_train[:,0] })\n",
        "        x = normalize(raw)\n",
        "        y = data.data_train[:,1]\n",
        "        return x,y\n",
        "\n",
        "    def get_base_test_data(self, seq_len, normalise):\n",
        "        raw = pd.DataFrame( {  data.columns[0]: data.data_test[:,0] })\n",
        "        x = normalize(raw)\n",
        "        y = data.data_test[:,1]\n",
        "        return x,y\n",
        "\n",
        "    def seq2seq_train_data(self, seq_len, normalise):\n",
        "        '''\n",
        "        Create x, y train data windows\n",
        "        Warning: batch method, not generative, make sure you have enough memory to\n",
        "        load data, otherwise use generate_training_window() method.\n",
        "        '''\n",
        "        data_x = []\n",
        "        data_y = []\n",
        "        for i in range(self.len_train - seq_len):\n",
        "            x, y = self._next_seq2seq_window(i, seq_len, normalise, self.data_train)\n",
        "            data_x.append(x)\n",
        "            data_y.append(y)\n",
        "        return np.array(data_x), np.array(data_y)\n",
        "\n",
        "    def generate_train_batch(self, seq_len, batch_size, normalise):\n",
        "        '''Yield a generator of training data from filename on given list of cols split for train/test'''\n",
        "        i = 0\n",
        "        while i < (self.len_train - seq_len):\n",
        "            x_batch = []\n",
        "            y_batch = []\n",
        "            for b in range(batch_size):\n",
        "                if i >= (self.len_train - seq_len):\n",
        "                    # stop-condition for a smaller final batch if data doesn't divide evenly\n",
        "                    yield np.array(x_batch), np.array(y_batch)\n",
        "                    i = 0\n",
        "                x, y = self._next_window(i, seq_len, normalise)\n",
        "                x_batch.append(x)\n",
        "                y_batch.append(y)\n",
        "                i += 1\n",
        "            yield np.array(x_batch), np.array(y_batch)\n",
        "\n",
        "    def get_normalised_column(self, column):\n",
        "              x = column[0]\n",
        "              if(x == 0):\n",
        "                x = window[1]\n",
        "                print(col_i, x)\n",
        "              normalised_col = [((float(p) / float(x)) - 1) for p in column]\n",
        "              return normalised_col\n",
        "\n",
        "    def get_normalized_column(self, column):\n",
        "      df = pd.DataFrame(column)\n",
        "      normalized_col = normalize(df).values\n",
        "      normalized_col = normalized_col.reshape(normalized_col.shape[0])\n",
        "      return normalized_col\n",
        "      \n",
        "\n",
        "    def normalise_windows(self, window_data, single_window=False):\n",
        "        '''Normalise window with a base value of zero'''\n",
        "        normalised_data = []\n",
        "        window_data = [window_data] if single_window else window_data\n",
        "        for window in window_data:\n",
        "            normalised_window = []\n",
        "#            for col_i in range(window.shape[1]-1): # not normalizing the last column (label)\n",
        "            for col_i in range(window.shape[1]): # use both columns as features\n",
        "                normalised_col = self.get_normalized_column(window[:,col_i])\n",
        "                normalised_window.append(normalised_col)\n",
        "            normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format\n",
        "            normalised_data.append(normalised_window)\n",
        "        return np.array(normalised_data)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}