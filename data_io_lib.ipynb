{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data.io.lib.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hossein20s/PPG2bloodPressure/blob/master/data_io_lib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyMsKDUYsVX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Version = '1.4'\n",
        "print('data.io.lib Version: ', Version)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghAaEfogGnaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def normalize(df):\n",
        "  #stats = dataframe.describe()\n",
        "  return (df - df.mean()) / df.std()\n",
        "\n",
        "\n",
        "def get_normalised_column(column):\n",
        "                x = column[0]\n",
        "                if(x == 0):\n",
        "                  x = window[1]\n",
        "                  print(col_i, x)\n",
        "                normalised_col = [((float(p) / float(x)) - 1) for p in column]\n",
        "                return normalised_col\n",
        "\n",
        "def get_normalized_column(column):\n",
        "  df = pd.DataFrame(column)\n",
        "  normalized_col = normalize(df).values\n",
        "  normalized_col = normalized_col.reshape(normalized_col.shape[0])\n",
        "  return normalized_col\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txfaUXEuJd2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "\n",
        "\n",
        "class DataLoader():\n",
        "    \"\"\"A class for loading and transforming data for the lstm model\"\"\"\n",
        "\n",
        "    def __init__(self, filename, split, cols, index=None):\n",
        "      if(filename.endswith('.csv')):\n",
        "        dataframe = pd.read_csv(filename)\n",
        "      elif(filename.endswith('.mat')):\n",
        "        with h5py.File(filename, 'r') as f:\n",
        "          print(list(f.keys()))\n",
        "          tmp = f['Part_1']\n",
        "          objectRef = tmp[0][0]\n",
        "          ds = f[objectRef]\n",
        "          dataframe = pd.DataFrame(index=index, columns=cols)\n",
        "          print(len(cols))\n",
        "          for i in range(len(cols)):\n",
        "            print(cols[i])\n",
        "            dataframe[cols[i]] = ds[:,i]\n",
        "          print(dataframe.shape)\n",
        "          dataframe = dataframe[:3000]\n",
        "          print(dataframe.shape)\n",
        "          print(dataframe.head(3))\n",
        "\n",
        "      i_split = int(len(dataframe) * split)\n",
        "      self.data_train = dataframe.get(cols).values[:i_split]\n",
        "      self.data_test  = dataframe.get(cols).values[i_split:]\n",
        "      self.len_train  = len(self.data_train)\n",
        "      self.len_test   = len(self.data_test)\n",
        "      self.len_train_windows = None\n",
        "\n",
        "    def get_test_data(self, seq_len, normalise):\n",
        "        '''\n",
        "        Create x, y test data windows\n",
        "        Warning: batch method, not generative, make sure you have enough memory to\n",
        "        load data, otherwise reduce size of the training split.\n",
        "        '''\n",
        "        data_windows = []\n",
        "        for i in range(self.len_test - seq_len):\n",
        "            data_windows.append(self.data_test[i:i+seq_len])\n",
        "\n",
        "        data_windows = np.array(data_windows).astype(float)\n",
        "        data_windows = self.normalise_windows(data_windows, single_window=False) if normalise else data_windows\n",
        "\n",
        "        x = data_windows[:, :-1]\n",
        "        y = data_windows[:, -1, [0]]\n",
        "        return x,y\n",
        "\n",
        "    def get_train_data(self, seq_len, normalise):\n",
        "        '''\n",
        "        Create x, y train data windows\n",
        "        Warning: batch method, not generative, make sure you have enough memory to\n",
        "        load data, otherwise use generate_training_window() method.\n",
        "        '''\n",
        "        data_x = []\n",
        "        data_y = []\n",
        "        for i in range(self.len_train - seq_len):\n",
        "            x, y = self._next_window(i, seq_len, normalise)\n",
        "            data_x.append(x)\n",
        "            data_y.append(y)\n",
        "        return np.array(data_x), np.array(data_y)\n",
        "\n",
        "    def generate_train_batch(self, seq_len, batch_size, normalise):\n",
        "        '''Yield a generator of training data from filename on given list of cols split for train/test'''\n",
        "        i = 0\n",
        "        while i < (self.len_train - seq_len):\n",
        "            x_batch = []\n",
        "            y_batch = []\n",
        "            for b in range(batch_size):\n",
        "                if i >= (self.len_train - seq_len):\n",
        "                    # stop-condition for a smaller final batch if data doesn't divide evenly\n",
        "                    yield np.array(x_batch), np.array(y_batch)\n",
        "                    i = 0\n",
        "                x, y = self._next_window(i, seq_len, normalise)\n",
        "                x_batch.append(x)\n",
        "                y_batch.append(y)\n",
        "                i += 1\n",
        "            yield np.array(x_batch), np.array(y_batch)\n",
        "\n",
        "    def _next_window(self, i, seq_len, normalise):\n",
        "        '''Generates the next data window from the given index location i'''\n",
        "        window = self.data_train[i:i+seq_len]\n",
        "        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
        "        x = window[:-1]\n",
        "        y = window[-1, [0]]\n",
        "        return x, y\n",
        "\n",
        "    def normalise_windows(self, window_data, single_window=False):\n",
        "        '''Normalise window with a base value of zero'''\n",
        "        normalised_data = []\n",
        "        window_data = [window_data] if single_window else window_data\n",
        "        for window in window_data:\n",
        "            normalised_window = []\n",
        "            for col_i in range(window.shape[1]):\n",
        "                normalised_col = get_normalized_column(window[:,col_i])\n",
        "                normalised_window.append(normalised_col)\n",
        "            normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format\n",
        "            normalised_data.append(normalised_window)\n",
        "        return np.array(normalised_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}